{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters and other variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the data type of the variable used for w\n",
    "dtype = torch.float\n",
    "dtypeVolume = torch.int\n",
    "\n",
    "# defines the device in which we would like to keep the tensor\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#definition of the Neural Network HyperParameters\n",
    "n_input = None\n",
    "n_hidden_layer = 5\n",
    "n_hidden = [10,20,5,15,14]\n",
    "n_out = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Used to enable training analysis through TensorBoard\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Hold the best model\n",
    "best_mse = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb\n",
    "import datetime\n",
    "#Importing the function getAllCurrencyMarketElement from the file DatabaseScript.ipynb\n",
    "from ipynb.fs.full.DatabaseScript import getAllCurrencyMarketElement\n",
    "#In entry we want a matrix with Open , High , Low , Close and Volume\n",
    "#In exit we want a array with Close \n",
    "t = getAllCurrencyMarketElement(\"BTC-USD\")\n",
    "\n",
    "date = t['Date']\n",
    "\n",
    "index = []\n",
    "\n",
    "for d in date:\n",
    "        index.append(datetime.datetime.strptime(d[2:10],\"%y-%m-%d\").date())\n",
    "    \n",
    "\n",
    "X = t.drop(columns='Date') #Entry in nn\n",
    "y = t['Close'] #Output of nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t--- X shape ---\n",
      "[TRAIN] : (356, 5) , [TEST] : (10, 5)\n",
      "\n",
      "\t--- y shape ---\n",
      "[TRAIN] : (356,) , [TEST] : (10,)\n",
      "\n",
      "TimeSeriesSplit(gap=0, max_train_size=None, n_splits=35, test_size=10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit , train_test_split\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=10,shuffle=False)\n",
    "\n",
    "print(f\"\\t--- X shape ---\\n[TRAIN] : {X_train.shape} , [TEST] : {X_test.shape}\")\n",
    "print(f\"\\n\\t--- y shape ---\\n[TRAIN] : {y_train.shape} , [TEST] : {y_test.shape}\\n\")\n",
    "\n",
    "taille_pred = 10\n",
    "nspl = X_train.shape[0]//taille_pred\n",
    "tscv = TimeSeriesSplit(n_splits=nspl, test_size=10)\n",
    "print(tscv)\n",
    "\n",
    "all_splits = list(tscv.split(X_train,y_train))\n",
    "\n",
    "\n",
    "# for i in range(len(all_splits)):\n",
    "#     print(f'Split {i} shape: {len(all_splits[i][0])}')\n",
    "# X_train.iloc[train_1]\n",
    "# X_train.iloc[valid_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definine NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_NN(n_input):\n",
    "    modules = []\n",
    "    modules.append(nn.Linear(in_features=n_input , out_features=n_hidden[0]))\n",
    "    modules.append(nn.ReLU())\n",
    "    for i in range(n_hidden_layer-1):\n",
    "                modules.append(nn.Linear(in_features=n_hidden[i] , out_features=n_hidden[i+1]))\n",
    "                modules.append(nn.ReLU())\n",
    "\n",
    "    modules.append(nn.Linear(in_features=n_hidden[-1], out_features=n_out))\n",
    "\n",
    "\n",
    "    model = nn.Sequential(*modules)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split training finished. Final loss: nan . Lowest loss : 2159185408.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 1731731840.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 1628892416.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 1480214272.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 995598976.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 883053376.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 883053376.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 802832768.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 425755328.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 406370688.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 406370688.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 406370688.000000 .\n",
      "Split training finished. Final loss: 1283641976029184.000000 . Lowest loss : 406370688.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 406370688.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 406370688.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 406370688.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 406370688.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 380903136.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 369118656.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 369118656.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 369118656.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 369118656.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 347861344.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 274017472.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 274017472.000000 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 274017472.000000 .\n",
      "Split training finished. Final loss: 0.119614 . Lowest loss : 0.119614 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 0.119614 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 0.119614 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 0.119614 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 0.119614 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 0.119614 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 0.119614 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 0.119614 .\n",
      "Split training finished. Final loss: nan . Lowest loss : 0.119614 .\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x356 and 346x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[19], line 38\u001b[0m\n",
      "\u001b[1;32m     34\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSplit training finished. Final loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.6f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m . Lowest loss : \u001b[39m\u001b[39m{\u001b[39;00mbest_mse\u001b[39m:\u001b[39;00m\u001b[39m.6f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m .\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m     37\u001b[0m X_train , y_test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(X_train[\u001b[39m'\u001b[39m\u001b[39mClose\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues,dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat) , torch\u001b[39m.\u001b[39mtensor(y_test\u001b[39m.\u001b[39mvalues,dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n",
      "\u001b[0;32m---> 38\u001b[0m y_pred \u001b[39m=\u001b[39m model(X_train)\n",
      "\u001b[1;32m     39\u001b[0m loss \u001b[39m=\u001b[39m loss_function(y_pred,y_test)\n",
      "\u001b[1;32m     41\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining finished. Final loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.6f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m . Lowest loss during training: \u001b[39m\u001b[39m{\u001b[39;00mbest_mse\u001b[39m}\u001b[39;00m\u001b[39m .\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n",
      "\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n",
      "\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n",
      "\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x356 and 346x10)"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "# training loop\n",
    "for split in all_splits:\n",
    "    i_train , i_valid = split\n",
    "    X_train_valid , y_valid = torch.tensor(X_train['Close'].iloc[i_train].values,dtype=torch.float) , torch.tensor(y_train.iloc[i_valid].values,dtype=torch.float)\n",
    "    \n",
    "\n",
    "    model = Create_NN(len(i_train))\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        y_pred = model(X_train_valid) \n",
    "        loss = loss_function(y_pred, y_valid)\n",
    "        if epoch % 100 == 0:\n",
    "            #print(f\"Epoch [{epoch+1:4}/{n_epochs}], loss: {loss:.6f}\")\n",
    "            # Write epoch loss for TensorBoard\n",
    "            writer.add_scalar(\"Loss/train\", loss.item(), epoch)\n",
    "        \n",
    "        #Maj best model\n",
    "        if loss.item() < best_mse:\n",
    "            best_mse = loss.item()\n",
    "            best_weights = model.parameters()\n",
    "\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "    \n",
    "    print(f\"Split training finished. Final loss: {loss:.6f} . Lowest loss : {best_mse:.6f} .\")\n",
    "\n",
    "\n",
    "X_train , y_test = torch.tensor(X_train['Close'].values,dtype=torch.float) , torch.tensor(y_test.values,dtype=torch.float)\n",
    "y_pred = model(X_train)\n",
    "loss = loss_function(y_pred,y_test)\n",
    "\n",
    "print(f\"Training finished. Final loss: {loss:.6f} . Lowest loss during training: {best_mse} .\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20632.41015625 22163.94921875 24197.53320312 24746.07421875\n",
      " 24375.9609375  25052.7890625  27423.9296875  26965.87890625\n",
      " 28038.67578125 28323.87304688]\n"
     ]
    }
   ],
   "source": [
    "print(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(1,n_epochs,n_epochs//100).astype(int)\n",
    "\n",
    "fig , axs = plt.subplots(2,2 , figsize=(16,10))\n",
    "fig.suptitle(\"Training Information and Results\")\n",
    "\n",
    "ax1 = axs[0,0]\n",
    "ax1.plot(a,history)\n",
    "ax1.set(xlabel=\"Epoch\" , ylabel=\"Loss\",title='Loss Function')\n",
    "\n",
    "final_result = y_train - y_pred\n",
    "ax2 = axs[0,1]\n",
    "ax2.plot(index,final_result.detach().numpy())\n",
    "ax2.set(ylabel='Difference',xlabel='x')\n",
    "ax2.set_title('Différence between original and predicted values')\n",
    "\n",
    "ax3 = axs[1,1]\n",
    "ax3.plot(index,y_train,x,y_pred.detach().numpy(),'r')\n",
    "ax3.set(title='Final trained result',ylabel='y_pred',xlabel='x')\n",
    "\n",
    "ax4 = axs[1,0]\n",
    "ax4.plot(index,y_train[0:700],'b',x,y_train[700:],'r')\n",
    "ax4.set(title='Initial values',ylabel='y_train',xlabel='x')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
